{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a70f560",
   "metadata": {},
   "source": [
    "# Developing a Feature Space fo a MultiLabel CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1296bcc",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, jaccard_score, hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from umap import UMAP\n",
    "from keras import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing user-defined modules\n",
    "from Scripts.ptbxl_ml import *\n",
    "from Scripts.lead2_ml import *\n",
    "# from Scripts.lead2_mc import get_cluster_accuracy as mc_cluster_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1726a",
   "metadata": {},
   "source": [
    "### Loading and Pre-Processing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c642267",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = r'Processed Datasets\\processed_dataset_12_lead_lr.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8453b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl, signals = load_database(npz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa74f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting signals and superclassses\n",
    "X = signals\n",
    "Y = ptbxl.loc[:, ['strat_fold', 'superclasses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce80108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "x_train = X[np.where(Y.strat_fold < 9)]\n",
    "y_train = Y[(Y.strat_fold < 9)].superclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "x_test = X[np.where(Y.strat_fold >= 9)]\n",
    "y_test = Y[(Y.strat_fold >= 9)].superclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = pd.DataFrame(data=x_test, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels into binary matrix format\n",
    "y_train, y_test, label_classes = ml_label_encoding(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7165672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330fe02",
   "metadata": {},
   "source": [
    "### Loading Model and Extracting Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path: str, model_type='a') -> keras.Sequential:\n",
    "    '''\n",
    "    Loads and returns a CNN model using pre-defined weights. The model generated and compiled is determined by the model \n",
    "    architecture specified for the 'model_type' argument.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: The path containing the pre-definded weights\n",
    "        model_type: The type of model architecture to compile\n",
    "        \n",
    "    Returns:\n",
    "        keras.Sequential: The compiled model\n",
    "    '''\n",
    "    # Generates a model using the article architecture\n",
    "    if model_type == 'a':\n",
    "        model = generate_article_1D_model()\n",
    "    \n",
    "    # Generates a model with using the default architecture\n",
    "    elif model_type == 'd':\n",
    "        model = generate_default_1D_model()\n",
    "    \n",
    "    elif model_type == 12:\n",
    "        model = generate_12lead_model()\n",
    "    \n",
    "    # Loads best weights from training session\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Compiles model with similar hyperparametes as pre-trained model\n",
    "    model.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "                  metrics = [keras.metrics.BinaryAccuracy(name='Accuracy'),\n",
    "                             keras.metrics.Recall(name='Recall'),\n",
    "                             keras.metrics.Precision(name='Precision'),\n",
    "                             keras.metrics.AUC(name='AUC')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(checkpoint_path: str, test_data: pd.DataFrame, model_type: str='a', show_summary: bool=True) -> np.ndarray:\n",
    "    '''\n",
    "    Extracts feature representations of a pre-trained model using validation data. The output (final) layer is removed from the \n",
    "    model in order to return the feature embeddings from the penultimate layer.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: The path containing the weights to load into the generated model\n",
    "        test_data: DataFrame containing the ECG signals corresponding to the testing/validation set\n",
    "        model_type: Specifies which model architecture to load\n",
    "        show_summary: Boolean value that determines if the new model architecture will be displayed\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A matrix containing feature embeddings from the penultimate layer\n",
    "    '''\n",
    "    # Loading model of specified architecture\n",
    "    model = load_model(checkpoint_path=checkpoint_path, model_type=model_type)\n",
    "    \n",
    "    # Removing the final layer and printing architecture of the model\n",
    "    model.pop()\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "   \n",
    "    # Predicts and extracts the features\n",
    "    feature_embeddings = model.predict(test_data)\n",
    "    \n",
    "    return feature_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d11501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with article architecture\n",
    "checkpoint_path = r'training_sessions\\session_6\\weights\\CustomCNN_12Lead_48.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f013651",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_embeddings = extract_features(checkpoint_path, x_test, model_type=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031769e",
   "metadata": {},
   "source": [
    "### Generating Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new model with complete article architecture\n",
    "model_1 = load_model(checkpoint_path, model_type=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7935c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_hat = model_predict(model=model_1,\n",
    "                              test_data=x_test,\n",
    "                              test_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictions:\\n', y_hat)\n",
    "print('\\nTrue Labels:\\n', y_test)\n",
    "print('\\nShape:', y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7856088",
   "metadata": {},
   "source": [
    "### Computing Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing the class labels extracted from the label encoding for the confusion matrix labels\n",
    "matrix_labels = label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fba40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = show_confusion_matrix(y_test, y_hat, classes=matrix_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c54b83",
   "metadata": {},
   "source": [
    "### Displaying Classification Report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_indices_report(true_labels: np.ndarray, pred_labels: np.ndarray, class_to_check: int) -> np.ndarray:\n",
    "    '''\n",
    "    Returns the indices where a false negative was computed for the specified class.\n",
    "    \n",
    "    Args:\n",
    "        true_labels: The ground truth labels\n",
    "        pred_labels: The predicted labels generated by a pre-trained model\n",
    "        class_to_check: The class that's being checked for false negatives\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: An array containing the indices of all the false negatives computed for the specified class\n",
    "    '''\n",
    "    error_indices = np.where((true_labels[:, class_to_check] == 1) & (pred_labels[:, class_to_check] != 1))[0]\n",
    "    \n",
    "    return error_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the error indices for all classes\n",
    "CD_error_indices = error_indices_report(y_test, y_hat, 0)\n",
    "HYP_error_indices = error_indices_report(y_test, y_hat, 1)\n",
    "MI_error_indices = error_indices_report (y_test, y_hat, 2)\n",
    "NORM_error_indices = error_indices_report(y_test, y_hat, 3)\n",
    "STTC_error_indicees = error_indices_report(y_test, y_hat, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_hat, target_names=['CD', 'HYP', 'MI', 'NORM', 'STTC']))\n",
    "\n",
    "print('False Negative CD Classifications:', len(CD_error_indices))\n",
    "print('False Negative HYP Classifications:', len(HYP_error_indices))\n",
    "print('False Negative MI Classifications:', len(MI_error_indices))\n",
    "print('False Negative NORM Classifications:', len(NORM_error_indices))\n",
    "print('False Negative STTC Classifications:', len(STTC_error_indicees))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ef6e2",
   "metadata": {},
   "source": [
    "### K-Means Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_class_clusters(feature_embeddings: np.ndarray, dim_red: str='p') -> np.ndarray:\n",
    "    '''\n",
    "    Returns clustered superclasses from the PTB-XL database based on the feature embeddings provided. \n",
    "    Clusters are computed via K-Means clustering.\n",
    "    \n",
    "    Args:\n",
    "        feature_embeddings: The feature embeddings to be clustered\n",
    "        dim_red: The dimensionality reduction to use when visualing clusters ('p'= PCA, 't'= TSNE, 'u'= UMAP)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array containing the assigned cluster label for each record/data point\n",
    "    '''\n",
    "    # Grouping extracted features into clusters and storing associated labels for all records\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42).fit(feature_embeddings)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Organzing data into a DataFrame\n",
    "    clusters = pd.DataFrame(data=labels, columns=['Cluster'])\n",
    "    \n",
    "    # Reducing data to two-dimensional feature space using the specified reduction technique\n",
    "    if dim_red == 'p':\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        data = pca.fit_transform(feature_embeddings)\n",
    "        col_names = ['pca1', 'pca2']\n",
    "        \n",
    "    elif dim_red == 't':\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        data = tsne.fit_transform(feature_embeddings)\n",
    "        col_names = ['tsne1', 'tsne2']\n",
    "    \n",
    "    elif dim_red == 'u':    \n",
    "        umap = UMAP(n_components=2, random_state=42)\n",
    "        data = umap.fit_transform(feature_embeddings)\n",
    "        col_names = ['umap1', 'umap2']\n",
    "\n",
    "    # Storing reduced data into a DataFrame and plotting the clusters\n",
    "    results = pd.DataFrame(data=data, columns=col_names) \n",
    "    sns.scatterplot(x=col_names[0],\n",
    "                    y=col_names[1],\n",
    "                    hue=clusters['Cluster'],\n",
    "                    data=results)\n",
    "    \n",
    "    plt.title('K-Means Clusters')\n",
    "    plt.show()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cluster_labels = display_class_clusters(feature_embeddings, dim_red='u')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5339d",
   "metadata": {},
   "source": [
    "### Clustering Based on Metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_feature(ptbxl_database: pd.DataFrame, train_data: pd.DataFrame, test_data: pd.DataFrame, feature: str, remove_na: bool=True, min_threshold: int | None=None, max_threshold: int | None=None) -> tuple[pd.Series, pd.Series]:\n",
    "    '''\n",
    "    Retrieves a specific feature from the PTB-XL database and returns both training and testing labels as Pandas Series. Removes any missing \n",
    "    values by default and can additionally remove any values below or above a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        ptbxl_database: The DataFrame containing all the PTB-XL metadata\n",
    "        train_data: The ECG signals corresponding to the train dataset\n",
    "        test_data: The ECG signals corresponding to the test/validation dataset\n",
    "        feature: The feature/attribute within 'ptbxl_database' that is being retrieved (e.g., age, height)\n",
    "        remove_na: Boolean value that determines if missing values will be removed\n",
    "        min_threshold: If specified, removes any records holding a value less than the threshold\n",
    "        max_threshold: If specified, removes any records holding a value larger than the threshold\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A two-element tuple (training and testing set) containing the labels for the specified feature\n",
    "    '''\n",
    "    # Retrieving record indices corresponding to both training and validation datasets\n",
    "    train_indices = train_data.index\n",
    "    test_indices = test_data.index\n",
    "    \n",
    "    # Acquiring labels corresponding to specified feature\n",
    "    train_labels = ptbxl_database.loc[train_indices, feature].convert_dtypes()\n",
    "    test_labels = ptbxl_database.loc[test_indices, feature].convert_dtypes()\n",
    "    \n",
    "    train_length = len(train_labels)\n",
    "    test_length = len(test_labels)\n",
    "    \n",
    "    if remove_na:\n",
    "        # Finding empty records\n",
    "        na_train_labels = pd.isna(train_labels)\n",
    "        na_test_labels = pd.isna(test_labels)\n",
    "        \n",
    "        # Keeping non-empty records\n",
    "        train_labels = train_labels[~(na_train_labels)]\n",
    "        test_labels = test_labels[~(na_test_labels)]\n",
    "        \n",
    "        print(f'Removed {train_length - len(train_labels)} records with missing values from training set')\n",
    "        print(f'Removed {test_length - len(test_labels)} records with missing values from testing set')\n",
    "        \n",
    "        # Updating length of both datasets\n",
    "        train_length = len(train_labels)\n",
    "        test_length = len(test_labels)\n",
    "        \n",
    "    if min_threshold:\n",
    "        # Removing values below minimum threshold\n",
    "        train_bool_mask = (train_labels >= min_threshold)\n",
    "        test_bool_mask = (test_labels >= min_threshold)\n",
    "        \n",
    "        train_labels = train_labels[train_bool_mask]\n",
    "        test_labels = test_labels[test_bool_mask]\n",
    "        \n",
    "        print('\\n')\n",
    "        print(f'Removed {train_length - len(train_labels)} records below threshold from training set')\n",
    "        print(f'Removed {test_length - len(test_labels)} records below threshold from testing set')\n",
    "        \n",
    "        # Updating length of both datasets\n",
    "        train_length = len(train_labels)\n",
    "        test_length = len(test_labels)\n",
    "        \n",
    "    if max_threshold:\n",
    "        # Removing values above max threshold\n",
    "        train_bool_mask = (train_labels <= max_threshold)\n",
    "        test_bool_mask = (test_labels <= max_threshold)\n",
    "        \n",
    "        train_labels = train_labels[train_bool_mask]\n",
    "        test_labels = test_labels[test_bool_mask]\n",
    "        \n",
    "        print('\\n')\n",
    "        print(f'Removed {train_length - len(train_labels)} records above threshold from training set')\n",
    "        print(f'Removed {test_length - len(test_labels)} records above threshold from testing set')\n",
    "        \n",
    "        # Updating length of both datasets\n",
    "        train_length = len(train_labels)\n",
    "        test_length = len(test_labels)\n",
    "    \n",
    "    train_labels_df = pd.Series(data=train_labels, index=train_labels.index)\n",
    "    test_labels_df = pd.Series(data=test_labels, index=test_labels.index)\n",
    "    \n",
    "    return (train_labels_df, test_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving age metadata\n",
    "age_train_df, age_test_df = get_metadata_feature(ptbxl, x_train, x_test, 'age', min_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb441ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving sex metadata\n",
    "sex_train_df, sex_test_df = get_metadata_feature(ptbxl, x_train, x_test, 'sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4187a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving height metadata\n",
    "height_train_df, height_test_df = get_metadata_feature(ptbxl, x_train, x_test, 'height', min_threshold=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving weight metadata\n",
    "weight_train_df, weight_test_df = get_metadata_feature(ptbxl, x_train, x_test, 'weight', min_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_buckets(train_metadata: pd.DataFrame | pd.Series, test_metadata: pd.DataFrame | pd.Series, buckets: int| list) -> tuple[pd.Series, pd.Series, np.ndarray]:\n",
    "    '''\n",
    "    Groups metadata into ordered buckets (i.e. categorically labeled groups).\n",
    "    \n",
    "    Args:\n",
    "        train_metadata: The DataFrame or Series holding the labels from the training set\n",
    "        test_metadata: The DataFrame or Series holding the labels from the testing/validation set        \n",
    "        buckets: As an integer, creates n equally sized buckets. As a list, uses list values as the ranges for each bucket\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: A three-element tuple containing: Training buckets, testing buckets, intervals used to create buckets\n",
    "    '''\n",
    "    if type(buckets) == int:\n",
    "        # Creating training buckets\n",
    "        train_buckets_df, train_bins = pd.cut(train_metadata,\n",
    "                                              bins=buckets,\n",
    "                                              labels=list(range(buckets)),\n",
    "                                              retbins=True)\n",
    "        \n",
    "        # Creating testing buckets\n",
    "        test_buckets_df = pd.cut(test_metadata,\n",
    "                                 bins=train_bins,\n",
    "                                 labels=list(range(buckets)))\n",
    "        \n",
    "    elif type(buckets) == list:\n",
    "        # Creating training buckets\n",
    "        train_buckets_df, train_bins = pd.cut(train_metadata,\n",
    "                                              bins=buckets,\n",
    "                                              labels=list(range(len(buckets) - 1)),\n",
    "                                              retbins=True,\n",
    "                                              include_lowest=True)\n",
    "        \n",
    "        # Creating testing buckets\n",
    "        test_buckets_df = pd.cut(test_metadata,\n",
    "                                 bins=buckets,\n",
    "                                 labels=list(range(len(buckets) - 1)),\n",
    "                                 include_lowest=True)\n",
    "    \n",
    "    # Printing bucket labels and their intervals\n",
    "    print(f'Bucket Intervals:\\n')\n",
    "    for i in range((len(train_bins) - 1)):\n",
    "        if i == 0:\n",
    "            print(f'Bucket {i}: [{train_bins[i]}, {train_bins[i+1]}]')\n",
    "        else:\n",
    "            print(f'Bucket {i}: ({train_bins[i]}, {train_bins[i+1]}]')\n",
    "                \n",
    "    return (train_buckets_df, test_buckets_df, train_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_buckets = [2, 25, 50, 75, np.inf]\n",
    "age_train_labels, age_test_labels, age_bucket_intervals = get_metadata_buckets(age_train_df, age_test_df, buckets=age_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_buckets = [60, 100, 133, 166, np.inf]\n",
    "height_train_labels, height_test_labels, height_bucket_intervals = get_metadata_buckets(height_train_df, height_test_df, buckets=height_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_buckets = [5, 60, 120, 180, np.inf]\n",
    "weight_train_labels, weight_test_labels, weight_bucket_intervals = get_metadata_buckets(weight_train_df, weight_test_df, buckets=weight_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362283bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature embeddings specific to each metadata feature\n",
    "age_test_indices = age_test_df.index\n",
    "age_feature_embeddings = extract_features(checkpoint_path, x_test.loc[age_test_indices], show_summary=False)\n",
    "\n",
    "sex_test_indices = sex_test_df.index\n",
    "sex_feature_embeddings = extract_features(checkpoint_path, x_test.loc[sex_test_indices], show_summary=False)\n",
    "\n",
    "height_test_indices = height_test_df.index\n",
    "height_feature_embeddings = extract_features(checkpoint_path, x_test.loc[height_test_indices], show_summary=False)\n",
    "\n",
    "weight_test_indices = weight_test_df.index\n",
    "weight_feature_embeddings = extract_features(checkpoint_path, x_test.loc[weight_test_indices], show_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc359299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metadata_clusters(metadata: pd.Series, feature_embeddings: np.ndarray, bucket_intervals: np.ndarray | None=None, dim_red: str='u'):\n",
    "    '''\n",
    "    Clusters feature embeddings via K-Means using the number of buckets/groups found in the data. The provided metadata should be\n",
    "    labeled according to its bucket group and consist of one of the features found within the PTB-XL database.\n",
    "    \n",
    "    Args:\n",
    "        metadata: The metadata (e.g., age, height) that will be used to cross-check the clustered feature embeddings\n",
    "        feature_embeddings: The feature embeddings that will be clustered\n",
    "        bucket_intervals: An array containing the ranges of all buckets regarding the metadata\n",
    "        dim_red: The dimensionality reduction to use when visualing clusters ('p'= PCA, 't'= TSNE, 'u'= UMAP)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array containing the assigned cluster labels for each record/data point\n",
    "    '''\n",
    "    # Retrieving the number of clusters\n",
    "    try:\n",
    "        n_clusters = len(bucket_intervals) - 1\n",
    "    except TypeError:\n",
    "        n_clusters = 2\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(feature_embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Reducing feature embeddings to a two-dimensional feature space using the specified reduction technique\n",
    "    if dim_red == 'p':\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        data = pca.fit_transform(feature_embeddings)\n",
    "        col_names = ['pca1', 'pca2']\n",
    "        \n",
    "    elif dim_red == 't':\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        data = tsne.fit_transform(feature_embeddings)\n",
    "        col_names = ['tsne1', 'tsne2']\n",
    "    \n",
    "    elif dim_red == 'u':    \n",
    "        umap = UMAP(n_components=2, random_state=42)\n",
    "        data = umap.fit_transform(feature_embeddings)\n",
    "        col_names = ['umap1', 'umap2']\n",
    "    \n",
    "    # Storing transformed data into a DataFrame and plotting the clusters\n",
    "    column_name = metadata.name.capitalize()\n",
    "   \n",
    "    results = pd.DataFrame(data=data, columns=col_names)\n",
    "    results[f'{column_name} Group'] = metadata.reset_index(drop=True)\n",
    "    results['Cluster'] = cluster_labels\n",
    "    \n",
    "    sns.scatterplot(x=col_names[0],\n",
    "                    y=col_names[1],\n",
    "                    hue=results['Cluster'],\n",
    "                    style=results[f'{column_name} Group'],\n",
    "                    data=results)\n",
    "    plt.title(f'K-Means Clusters vs. {column_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cluster_labels = display_metadata_clusters(age_test_labels, age_feature_embeddings, age_bucket_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_cluster_labels = display_metadata_clusters(sex_test_df, sex_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_cluster_labels = display_metadata_clusters(height_test_labels, height_feature_embeddings, height_bucket_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d69589",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_labels = display_metadata_clusters(weight_test_labels, weight_feature_embeddings, weight_bucket_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86361aea",
   "metadata": {},
   "source": [
    "### Evaluating Clustering Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_accuracy(test_labels: pd.DataFrame | pd.Series | np.ndarray, cluster_labels: np.ndarray, n_classes: int) -> dict:\n",
    "    '''\n",
    "    Determines the accuracy between the ground truth labels and the assigned cluster labels using a two-method approach. The first\n",
    "    method creates prototypical vectors for each cluster. These vectors are averaged and binarized to represent the most prominent\n",
    "    classes found within a cluster. This vector-class mapping is then utilized for the second method. The second method computes an \n",
    "    exact match score, a Jaccard score, and the Hamming loss. Confusion matrices are then used to determine the best class-cluster pair.\n",
    "    \n",
    "    Args:\n",
    "        test_labels: Ground truth labels of shape (n_samples, n_classes)\n",
    "        cluster_labels: An array containing cluster label assignments\n",
    "        n_classes: The number of unique classes within the test labels\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing various metrics (e.g., exact match score (clustering accuracy),\n",
    "        Jaccard Score, and Hamming Loss)\n",
    "    '''\n",
    "    # Method 1: Prototypical vectors for each cluster\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    \n",
    "    # Creating zero-array to hold prototypical vectors for each cluster\n",
    "    if n_classes > 2:\n",
    "        cluster_vectors = np.zeros((n_clusters, n_classes))\n",
    "    else:\n",
    "        cluster_vectors = np.zeros((n_clusters, 1))\n",
    "        \n",
    "    for cluster_id in range(n_clusters):\n",
    "        # Creating boolean mask for current cluster\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_data = test_labels[cluster_mask]\n",
    "        cluster_data = cluster_data.astype(int)\n",
    "        \n",
    "        # Averaging data to get singular vector representation of current cluster\n",
    "        if len(cluster_data) > 0:\n",
    "            cluster_vectors[cluster_id] = np.mean(cluster_data, axis=0)\n",
    "        \n",
    "    # Binarizing vector to keep most prominent labels/classes\n",
    "    cluster_vectors = (cluster_vectors >= 0.5).astype(int)\n",
    "\n",
    "    # Generating predictions based on vector-label mapping\n",
    "    predictions = cluster_vectors[cluster_labels]\n",
    "            \n",
    "    # Method 2: Per-class best cluster mapping\n",
    "    per_class_mappings = {}\n",
    "    per_class_accuracies = {}\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        \n",
    "        # Retrieving binary class labels (i.e., 0 or 1) for current class\n",
    "        if (n_classes > 2 ) and (isinstance(test_labels, (np.ndarray, pd.DataFrame))):\n",
    "            if isinstance(test_labels, np.ndarray):\n",
    "                binary_class_labels = test_labels[:, class_idx]\n",
    "            elif isinstance(test_labels, pd.DataFrame):\n",
    "                binary_class_labels = test_labels.iloc[:, class_idx]\n",
    "        else:\n",
    "            binary_class_labels = (test_labels == class_idx).astype(int)\n",
    "        \n",
    "        class_conf_matrices = []\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            # Retrieving binary cluster labels (i.e., 0 or 1) for current cluster\n",
    "            binary_cluster_labels = (cluster_labels == cluster_id).astype(int)\n",
    "            \n",
    "            # Creating a 2x2 matrix for current class and cluster\n",
    "            tn = np.sum((binary_class_labels == 0) & (binary_cluster_labels == 0))\n",
    "            fp = np.sum((binary_class_labels == 0) & (binary_cluster_labels == 1))\n",
    "            fn = np.sum((binary_class_labels == 1) & (binary_cluster_labels == 0))\n",
    "            tp = np.sum((binary_class_labels == 1) & (binary_cluster_labels == 1))\n",
    "            \n",
    "            # Storing positive assignment scores (i.e., true positives and true negatives)\n",
    "            class_conf_matrices.append(tp + tn)\n",
    "        \n",
    "        # Determining the best cluster-class pair  \n",
    "        best_cluster_idx = np.argmax(class_conf_matrices)\n",
    "        per_class_mappings[f'Class {class_idx}'] = best_cluster_idx\n",
    "        per_class_accuracies[f'Class {class_idx}'] = (class_conf_matrices[best_cluster_idx] / len(test_labels))\n",
    "            \n",
    "    # Calculating Jaccard score and Hamming loss\n",
    "    jaccard = jaccard_score(test_labels, predictions, average='macro')\n",
    "    hamming = hamming_loss(test_labels, predictions)\n",
    "    \n",
    "    # Calculating exact match accuracy\n",
    "    if n_classes > 2:\n",
    "        exact_match_array = np.all(test_labels == predictions, axis=1)\n",
    "    else:\n",
    "        exact_match_array = (test_labels == predictions.flatten())\n",
    "\n",
    "    exact_match_score = np.mean(exact_match_array)\n",
    "    \n",
    "    results = {'cluster_vectors': cluster_vectors,\n",
    "               'per_class_best_cluster': per_class_mappings,\n",
    "               'per_class_accuracies': per_class_accuracies,\n",
    "               'exact_match_accuracy': exact_match_score,\n",
    "               'jaccard_score': jaccard,\n",
    "               'hamming_loss': hamming,\n",
    "               'predictions': predictions}\n",
    "    \n",
    "    print(f'Clustering Accuracy: {exact_match_score:.2%}')\n",
    "    print('Per-Class Best Clusters:', per_class_mappings)\n",
    "    print(f'Jaccard Score: {jaccard:.2f}')\n",
    "    print(f'Hamming Loss: {hamming:.2f}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ffd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining clustering accuracy for superclass labels\n",
    "class_cluster_results = get_cluster_accuracy(y_test, class_cluster_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1631e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cluster_accuracy = mc_cluster_acc(age_test_labels, age_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_cluster_accuracy = mc_cluster_acc(sex_test_df, sex_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_cluster_accuracy = mc_cluster_acc(height_test_labels, height_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_accuracy = mc_cluster_acc(weight_test_labels, weight_cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9a375",
   "metadata": {},
   "source": [
    "### Implementing a Probing Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f246778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_probing_classifier(model: keras.Sequential, train_data: pd.DataFrame, test_data: pd.DataFrame, n_classes: int, layer_to_probe: str | int=-2) -> tuple[keras.Sequential, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Generates a probing classifier based off the model provided. Training and testing features are created by utilizing an intermediate\n",
    "    model to generate predictions. These features are then returned and can then be used to train/fit the compiled classifier.\n",
    "    \n",
    "    Args:\n",
    "        model: The pre-trained model that will be analyzed\n",
    "        train_data: The training data used for the original model\n",
    "        test_data: The testing/validation data used for the original model\n",
    "        n_classes: The number of unique classes/labels\n",
    "        layer_to_probe: The layer from the original model to probe (defaults to the penultimate layer)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A three-element tuple containing the compiled probing classifier, its training features, and its testing features\n",
    "    '''\n",
    "    # Freezing the model\n",
    "    model.trainable = False\n",
    "    \n",
    "    # Defining the input and ouput layer for a smaller, intermediate model\n",
    "    input_layer = model.layers[0].input\n",
    "    \n",
    "    if isinstance(layer_to_probe, str):\n",
    "        output_layer = model.get_layer(layer_to_probe).output\n",
    "    else:\n",
    "        output_layer = model.get_layer(index=layer_to_probe).output\n",
    "        \n",
    "    # Building the intermediate model\n",
    "    intermediate_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Retrieving the feature embeddings for both training and validations sets\n",
    "    train_features = intermediate_model.predict(train_data)\n",
    "    test_features = intermediate_model.predict(test_data)\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        # Defining the probe classifier's architecture\n",
    "        probe_classifier = Sequential([Input(shape=(train_features.shape[1],)),\n",
    "                                       Dense(1, activation='sigmoid')])\n",
    "        \n",
    "        # Compiling the classifier\n",
    "        probe_classifier.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                                 optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "                                 metrics = [keras.metrics.BinaryAccuracy(name='Accuracy')])\n",
    "        \n",
    "    elif n_classes > 2:\n",
    "        # Defining the probe classifier's architecture\n",
    "        probe_classifier = Sequential([Input(shape=(train_features.shape[1],)),\n",
    "                                       Dense(n_classes, activation='softmax')])\n",
    "        \n",
    "        # Compiling the classifier\n",
    "        probe_classifier.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                 optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "                                 metrics = [keras.metrics.SparseCategoricalAccuracy(name='Accuracy')])\n",
    "    \n",
    "    return (probe_classifier, train_features, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating probing classifier specific to each and its training/testing features\n",
    "age_classes = np.unique(age_train_labels)\n",
    "\n",
    "age_compiled_classifier, age_train_features, age_test_features = generate_probing_classifier(model_1,\n",
    "                                                                                             x_train.loc[age_train_labels.index],\n",
    "                                                                                             x_test.loc[age_test_labels.index],\n",
    "                                                                                             n_classes=len(age_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_compiled_classifier, sex_train_features, sex_test_features = generate_probing_classifier(model_1,\n",
    "                                                                                              x_train.loc[sex_train_df.index],\n",
    "                                                                                              x_test.loc[sex_test_df.index],\n",
    "                                                                                              n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de388607",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_classes = np.unique(height_train_labels)\n",
    "\n",
    "height_compiled_classifier, height_train_features, height_test_features = generate_probing_classifier(model_1,\n",
    "                                                                                                      x_train.loc[height_train_labels.index],\n",
    "                                                                                                      x_test.loc[height_test_labels.index],\n",
    "                                                                                                      n_classes=len(height_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_classes = np.unique(weight_train_labels)\n",
    "\n",
    "weight_compiled_classifier, weight_train_features, weight_test_features = generate_probing_classifier(model_1,\n",
    "                                                                                                      x_train.loc[weight_train_labels.index],\n",
    "                                                                                                      x_test.loc[weight_test_labels.index],\n",
    "                                                                                                      n_classes=len(weight_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe_classifier(probe_model: keras.Sequential, train_features: np.ndarray, train_labels: np.ndarray, test_features: np.ndarray, test_labels: np.ndarray, save_path: str, epochs: int=50) -> tuple:\n",
    "    '''\n",
    "    Trains a compiled classifier based on the training and testing features provided.\n",
    "    \n",
    "    Args:\n",
    "        probe_model: A compiled probing classifier\n",
    "        train_features: Feature embeddings to be used for training\n",
    "        train_labels: Labels corresponding the training set\n",
    "        test_features: Feature embeddings to be used for testing/validation\n",
    "        test_labels: Labels corresponding to the testing set\n",
    "        save_path: Location to save the probing classifier's best weights\n",
    "        epochs: The number of epochs to train the classifier for\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A two-element tuple containing the training history of the classifier and the trained classifier itself\n",
    "    '''\n",
    "    # Defining checkpoint parameters\n",
    "    probe_callbacks = [keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                       monitor='val_Accuracy',\n",
    "                                                       verbose=1,\n",
    "                                                       save_best_only=True,\n",
    "                                                       save_weights_only=True,\n",
    "                                                       mode='max')]\n",
    "    \n",
    "    # Ensuring training and testing labels are in readable format\n",
    "    train_labels = train_labels.astype('int32')\n",
    "    test_labels = test_labels.astype('int32')\n",
    "    \n",
    "    # Training the classifier\n",
    "    history = probe_model.fit(train_features,\n",
    "                              train_labels,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=(test_features, test_labels),\n",
    "                              callbacks=probe_callbacks)\n",
    "    \n",
    "    return (history, probe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining save path\n",
    "save_path = r'Probe Classifier\\Multilabel\\Age\\50_epochs\\Weights\\Probe_Classifier_{epoch}.weights.h5'\n",
    "\n",
    "# Training classifier on age metadata\n",
    "age_history, age_trained_classifier = train_probe_classifier(probe_model=age_compiled_classifier,\n",
    "                                                             train_features=age_train_features,\n",
    "                                                             train_labels=age_train_labels,\n",
    "                                                             test_features=age_test_features,\n",
    "                                                             test_labels=age_test_labels,\n",
    "                                                             save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d96feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(age_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e900ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining save path\n",
    "save_path = r'Probe Classifier\\Multilabel\\Sex\\50_epochs\\Weights\\Probe_Classifier_{epoch}.weights.h5'\n",
    "\n",
    "# Training classifier on sex metadata\n",
    "sex_history, sex_trained_classifier = train_probe_classifier(probe_model=sex_compiled_classifier,\n",
    "                                                             train_features=sex_train_features,\n",
    "                                                             train_labels=sex_train_df,\n",
    "                                                             test_features=sex_test_features,\n",
    "                                                             test_labels=sex_test_df,\n",
    "                                                             save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10caf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(sex_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ae572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining save path\n",
    "save_path = r'Probe Classifier\\Multilabel\\Height\\50_epochs\\Weights\\Probe_Classifier_{epoch}.weights.h5'\n",
    "\n",
    "# Training classifier on height metadata\n",
    "height_history, height_trained_classifier = train_probe_classifier(probe_model=height_compiled_classifier,\n",
    "                                                                   train_features=height_train_features,\n",
    "                                                                   train_labels=height_train_labels,\n",
    "                                                                   test_features=height_test_features,\n",
    "                                                                   test_labels=height_test_labels,\n",
    "                                                                   save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(height_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeae222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining save path\n",
    "save_path = r'Probe Classifier\\Multilabel\\Weight\\50_epochs\\Weights\\Probe_Classifier_{epoch}.weights.h5'\n",
    "\n",
    "# Training classifier on weight metadata\n",
    "weight_history, weight_trained_classifier = train_probe_classifier(probe_model=weight_compiled_classifier,\n",
    "                                                                   train_features=weight_train_features,\n",
    "                                                                   train_labels=weight_train_labels,\n",
    "                                                                   test_features=weight_test_features,\n",
    "                                                                   test_labels=weight_test_labels,\n",
    "                                                                   save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(weight_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_score(probe_model: keras.Sequential, test_features: np.ndarray, test_labels: np.ndarray) -> list:\n",
    "    '''\n",
    "    Returns the loss and accuracy scores of a trained probing classifier.\n",
    "    \n",
    "    Args:\n",
    "        probe_model: The pre-trained probing classifier\n",
    "        test_features: The feature embeddings used for testing/validation\n",
    "        test_labels: The labels corresponding the test features\n",
    "        \n",
    "    Returns:\n",
    "        list: A two-element list containing the loss and accuracy of the probing classifier\n",
    "    '''\n",
    "    # Ensuring testing labels are in readable format\n",
    "    test_labels = test_labels.astype('int32')\n",
    "    \n",
    "    score = evaluate_model(probe_model, test_features, test_labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_score = get_probe_score(age_trained_classifier, age_test_features, age_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b460ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_score = get_probe_score(sex_trained_classifier, sex_test_features, sex_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_score = get_probe_score(height_trained_classifier, height_test_features, height_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = get_probe_score(weight_trained_classifier, weight_test_features, weight_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef350f8",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy of Probing Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best weights for each feature-specific classifier\n",
    "age_compiled_classifier.load_weights(r'Probe Classifier\\Multilabel\\Age\\50_epochs\\Weights\\Probe_Classifier_18.weights.h5')\n",
    "\n",
    "sex_compiled_classifier.load_weights(r'Probe Classifier\\Multilabel\\Sex\\50_epochs\\Weights\\Probe_Classifier_50.weights.h5')\n",
    "\n",
    "height_compiled_classifier.load_weights(r'Probe Classifier\\Multilabel\\Height\\50_epochs\\Weights\\Probe_Classifier_11.weights.h5')\n",
    "\n",
    "weight_compiled_classifier.load_weights(r'Probe Classifier\\Multilabel\\Weight\\50_epochs\\Weights\\Probe_Classifier_40.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for age groups\n",
    "age_probe_pred = age_compiled_classifier.predict(age_test_features)\n",
    "age_probe_pred = np.argmax(age_probe_pred, axis=1)\n",
    "print('\\nPredictions:\\n', age_probe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_acc = accuracy_score(age_test_labels, age_probe_pred)\n",
    "print(f'Age Accuracy Score: {age_acc:.2%}\\n')\n",
    "print('Classification Report:\\n\\n', classification_report(age_test_labels, age_probe_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94085059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for sex groups\n",
    "sex_probe_pred = sex_compiled_classifier.predict(sex_test_features)\n",
    "sex_probe_pred = (sex_probe_pred >= 0.5).astype(int).flatten()\n",
    "print('\\nPredictions:\\n', sex_probe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_acc = accuracy_score(sex_test_df, sex_probe_pred)\n",
    "print(f'Sex Accuracy Score: {sex_acc:.2%}\\n')\n",
    "print('Classification Report:\\n\\n', classification_report(sex_test_df, sex_probe_pred, target_names=['Male', 'Female']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c079b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for height groups\n",
    "height_probe_pred = height_compiled_classifier.predict(height_test_features)\n",
    "height_probe_pred = np.argmax(height_probe_pred, axis=1)\n",
    "print('\\nPredictions:\\n', height_probe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33383c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_acc = accuracy_score(height_test_labels, height_probe_pred)\n",
    "print(f'Height Accuracy Score: {height_acc:.2%}\\n')\n",
    "print('Classification Report:\\n\\n', classification_report(height_test_labels, height_probe_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66344acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for weight groups\n",
    "weight_probe_pred = weight_compiled_classifier.predict(weight_test_features)\n",
    "weight_probe_pred = np.argmax(weight_probe_pred, axis=1)\n",
    "print('\\nPredictions:\\n', weight_probe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_acc = accuracy_score(weight_test_labels, weight_probe_pred)\n",
    "print(f'Weight Accuracy Score: {weight_acc:.2%}\\n')\n",
    "print('Classification Report:\\n\\n', classification_report(weight_test_labels, weight_probe_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
